{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "424d8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75024d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_train = pd.read_csv(\"metadata.csv\",header=0,index_col=0)\n",
    "microbiome_train = pd.read_csv(\"microbiome.csv\",header=0,index_col=0)\n",
    "metabolome_train = pd.read_csv(\"serum_lipo.csv\",header=0,index_col=0)\n",
    "metadata_test = pd.read_csv(\"metadata_test.csv\",header=0,index_col=0)\n",
    "microbiome_test = pd.read_csv(\"microbiome_test.csv\",header=0,index_col=0)\n",
    "metabolome_test = pd.read_csv(\"serum_lipo_test.csv\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "edcf544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clr_transform(df: pd.DataFrame, epsilon=1e-6) -> pd.DataFrame:\n",
    "    clr_df = df.fillna(0.0).astype(float).values\n",
    "    clr_df = np.log(clr_df + epsilon)\n",
    "    clr_df = clr_df - clr_df.mean(axis=1, keepdims=True)\n",
    "    return pd.DataFrame(clr_df, index=df.index, columns=df.columns)\n",
    "def standartization_transform(train_df, test_df):\n",
    "    scaler = StandardScaler().fit(train_df)\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train_df), index=train_df.index, columns=train_df.columns)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test_df), index=test_df.index, columns=test_df.columns)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d88c5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "microbiome_train = clr_transform(microbiome_train)\n",
    "metadata_train['Disease_status'] = metadata_train['PATGROUPFINAL_C'].apply(lambda x:0 if x == '8' else 1)\n",
    "labels_train = metadata_train['Disease_status']\n",
    "microbiome_test = clr_transform(microbiome_test)\n",
    "metabolome_train, metabolome_test = standartization_transform(metabolome_train, metabolome_test)\n",
    "microbiome_and_metabolome_train = pd.merge(metabolome_train, microbiome_train, left_index=True, right_index=True, how='inner')\n",
    "microbiome_and_metabolome_test = pd.merge(metabolome_test, microbiome_test, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3aad4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With mean between bags\n",
    "def run_bagged_random_forest(train_data, train_labels, test_data):\n",
    "    rand = np.random.default_rng()\n",
    "    # labels: 0 = healthy, 1 = sick\n",
    "    healthy_indices = np.where(train_labels == 0)[0]\n",
    "    sick_indices    = np.where(train_labels == 1)[0]\n",
    "    n_bags = 60\n",
    "    target_sick_ratio = 0.1\n",
    "\n",
    "    # Options for model to increase diversity between bags\n",
    "    bag_size_low  = 150\n",
    "    bag_size_high = 250\n",
    "    max_features_pool   = [\"sqrt\", 0.2, 0.33, 0.5]\n",
    "    max_depth_pool      = [4, 6, 8, 10, 12, None]\n",
    "    min_samples_leaf_pool = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    probs_train_sum = np.zeros(len(train_labels), dtype=float)\n",
    "    probs_test_sum  = np.zeros(len(test_data), dtype=float)\n",
    "    probs_train_bags = []\n",
    "    probs_test_bags  = []\n",
    "\n",
    "    for b in range(n_bags):\n",
    "        # Different bag size each time\n",
    "        bag_size = int(rand.integers(bag_size_low, bag_size_high + 1))\n",
    "        n_sick_bag    = max(1, int(round(target_sick_ratio * bag_size)))\n",
    "        n_healthy_bag = max(1, bag_size - n_sick_bag)\n",
    "\n",
    "        # Sampling to have 90% healthy 10% sick inside each bag\n",
    "        sick_sample = rand.choice(\n",
    "            sick_indices,\n",
    "            size=n_sick_bag,\n",
    "            replace=(n_sick_bag > len(sick_indices))\n",
    "        )\n",
    "        healthy_sample = rand.choice(\n",
    "            healthy_indices,\n",
    "            size=n_healthy_bag,\n",
    "            replace=True\n",
    "        )\n",
    "\n",
    "        bag_indices = np.concatenate([healthy_sample, sick_sample])\n",
    "        rand.shuffle(bag_indices)\n",
    "\n",
    "        data_bag   = train_data.iloc[bag_indices]\n",
    "        labels_bag = train_labels.iloc[bag_indices]\n",
    "\n",
    "        # Choose model hyperparameters for current bag\n",
    "        max_features_choice   = max_features_pool[int(rand.integers(0, len(max_features_pool)))]\n",
    "        max_depth_choice      = max_depth_pool[int(rand.integers(0, len(max_depth_pool)))]\n",
    "        min_samples_leaf_choice = min_samples_leaf_pool[int(rand.integers(0, len(min_samples_leaf_pool)))]\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            max_features=max_features_choice,\n",
    "            max_depth=max_depth_choice,\n",
    "            min_samples_leaf=min_samples_leaf_choice,\n",
    "        )\n",
    "        model.fit(data_bag, labels_bag)\n",
    "\n",
    "        # Prediction probabilities per bag\n",
    "        probs_train = model.predict_proba(train_data)[:, 1]\n",
    "        probs_test  = model.predict_proba(test_data)[:, 1]\n",
    "\n",
    "        probs_train_sum += probs_train\n",
    "        probs_test_sum  += probs_test\n",
    "\n",
    "        probs_train_bags.append(probs_train)\n",
    "        probs_test_bags.append(probs_test)\n",
    "\n",
    "    # Average probabilities across bags\n",
    "    probs_train_mean = probs_train_sum / n_bags\n",
    "    probs_test_mean  = probs_test_sum  / n_bags\n",
    "\n",
    "    probs_train_bags = np.vstack(probs_train_bags)\n",
    "    probs_test_bags  = np.vstack(probs_test_bags)\n",
    "    \n",
    "    return probs_train_mean, probs_test_mean, probs_train_bags, probs_test_bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15336e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_layer(train_data, train_labels, test_data):\n",
    "    train_probs, test_probs, train_probs_bags, test_probs_bags = run_bagged_random_forest(train_data, train_labels, test_data)\n",
    "\n",
    "    std_train = train_probs_bags.std(axis=0)\n",
    "    std_test = test_probs_bags.std(axis=0)\n",
    "    prob_lower, prob_upper = np.quantile(train_probs, [0.02, 0.6])\n",
    "    std_threshold = np.quantile(std_train, 0.4)\n",
    "\n",
    "    certain_test_samples_mask = ((test_probs <= prob_lower) | (test_probs >= prob_upper)) & (std_test <= std_threshold)\n",
    "    uncertain_test_samples_mask = ~certain_test_samples_mask\n",
    "    certain_indices = np.where(certain_test_samples_mask)[0]\n",
    "    uncertain_indices  = np.where(uncertain_test_samples_mask)[0]\n",
    "    test_data_uncertain = test_data.iloc[uncertain_indices]\n",
    "    test_probs_certain = test_probs[certain_indices]\n",
    "    return test_data_uncertain, test_probs_certain, test_probs[uncertain_indices], test_data.index[certain_indices], test_data.index[uncertain_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89368c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgbm_with_reweighting(train_data, train_labels, test_data):\n",
    "    \"\"\"\n",
    "    Fits a LightGBM classifier with sample weights to correct for class imbalance.\n",
    "    The function assumes an initial distribution of 90% sick / 10% healthy in the training data\n",
    "    and re-weights it to a target of 10% sick / 90% healthy.\n",
    "    \"\"\"\n",
    "    target_ratio = 0.1\n",
    "    sick_indices = train_labels == 1\n",
    "    healthy_indices = train_labels == 0\n",
    "    sick_num = sick_indices.sum()\n",
    "    healthy_num = healthy_indices.sum()\n",
    "\n",
    "    w_sick = 1.0\n",
    "    w_healthy = (w_sick * sick_num * (1 - target_ratio)) / (target_ratio * healthy_num)\n",
    "    \n",
    "    # Assign weights per sample\n",
    "    weights = np.ones_like(train_labels, dtype=float)\n",
    "    weights[sick_indices] = w_sick\n",
    "    weights[healthy_indices] = w_healthy\n",
    "\n",
    "    lgbm = lgb.LGBMClassifier(objective='binary', verbose=-1)\n",
    "    lgbm.fit(train_data, train_labels, sample_weight=weights)\n",
    "\n",
    "    probs_test_uncertain = lgbm.predict_proba(test_data)[:, 1]\n",
    "    return probs_test_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ae89ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_layer(train_data, train_labels, test_data_uncertain):\n",
    "    # Run the model on the uncertain data from the first layer\n",
    "    probs_uncertain = run_lgbm_with_reweighting(train_data, train_labels, test_data_uncertain)\n",
    "    return probs_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c46611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(train_data, train_labels, test_data):\n",
    "    test_data_uncertain, test_probs_certain, test_probs_uncertain_layer1, test_certain_indices, test_uncertain_indices = first_layer(train_data, train_labels, test_data)\n",
    "    test_probs_uncertain = second_layer(train_data, train_labels, test_data_uncertain)\n",
    "    \n",
    "    confidence_layer1 = np.abs(test_probs_uncertain_layer1 - 0.5)\n",
    "    confidence_layer2 = np.abs(test_probs_uncertain - 0.5)\n",
    "    final_uncertain = np.where(confidence_layer2 > confidence_layer1 , test_probs_uncertain, test_probs_uncertain_layer1)\n",
    "    final_probs = pd.Series(index=test_data.index, dtype=float)\n",
    "    final_probs.loc[test_certain_indices]  = test_probs_certain\n",
    "    final_probs.loc[test_uncertain_indices] = final_uncertain\n",
    "    df = final_probs.to_frame(name=\"Probability\")\n",
    "    df.index.name = \"ID\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = two_layer_model(microbiome_and_metabolome_train, labels_train, microbiome_and_metabolome_test)\n",
    "results.to_csv(\"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
